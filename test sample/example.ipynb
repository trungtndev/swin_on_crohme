{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T09:25:29.075230Z",
     "start_time": "2024-10-05T09:25:24.725726Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from numpy.ma.core import shape\n",
    "import io\n",
    "\n",
    "from swinArm.datamodule import vocab\n",
    "from swinArm.lit_swinPreArm import LitSwinPreARM\n",
    "from torchvision.transforms import ToTensor, Normalize, Resize, Compose\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from IPython.display import display"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T09:25:42.994094Z",
     "start_time": "2024-10-05T09:25:42.963551Z"
    }
   },
   "source": "ckpt = '../pretrain_model/lasted-v1.ckpt'",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T09:25:29.106480Z",
     "start_time": "2024-10-05T09:25:29.075230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "comp = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    Resize((224, 224)),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T16:08:41.856314Z",
     "start_time": "2024-10-04T16:08:41.643461Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T09:26:15.303753Z",
     "start_time": "2024-10-05T09:26:14.152605Z"
    }
   },
   "source": [
    "model = LitSwinPreARM.load_from_checkpoint(ckpt)\n",
    "model = model.eval()\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T09:31:59.549135Z",
     "start_time": "2024-10-05T09:31:59.533508Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitSwinPreARM(\n",
       "  (model): SwinPreARM(\n",
       "    (encoder): SwinV1Encoder(\n",
       "      (swinv1): SwinTransformer(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (layers): Sequential(\n",
       "          (0): SwinTransformerStage(\n",
       "            (downsample): Identity()\n",
       "            (blocks): Sequential(\n",
       "              (0): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "                  (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.1, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                  (drop2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (1): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "                  (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.027)\n",
       "                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.1, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                  (drop2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.027)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerStage(\n",
       "            (downsample): PatchMerging(\n",
       "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "            )\n",
       "            (blocks): Sequential(\n",
       "              (0): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "                  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.055)\n",
       "                (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.1, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                  (drop2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.055)\n",
       "              )\n",
       "              (1): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "                  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.082)\n",
       "                (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.1, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                  (drop2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.082)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerStage(\n",
       "            (downsample): PatchMerging(\n",
       "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "            )\n",
       "            (blocks): Sequential(\n",
       "              (0): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.109)\n",
       "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.1, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (drop2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.109)\n",
       "              )\n",
       "              (1): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.136)\n",
       "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.1, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (drop2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.136)\n",
       "              )\n",
       "              (2): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.164)\n",
       "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.1, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (drop2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.164)\n",
       "              )\n",
       "              (3): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.191)\n",
       "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.1, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (drop2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.191)\n",
       "              )\n",
       "              (4): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.218)\n",
       "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.1, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (drop2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.218)\n",
       "              )\n",
       "              (5): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.245)\n",
       "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.1, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (drop2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.245)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerStage(\n",
       "            (downsample): PatchMerging(\n",
       "              (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "            )\n",
       "            (blocks): Sequential(\n",
       "              (0): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "                  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.273)\n",
       "                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.1, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (drop2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.273)\n",
       "              )\n",
       "              (1): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "                  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.300)\n",
       "                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.1, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (drop2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.300)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (head): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (word_embed): Sequential(\n",
       "        (0): Embedding(113, 256)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_enc): WordPosEnc()\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (model): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.3, inplace=False)\n",
       "            (dropout2): Dropout(p=0.3, inplace=False)\n",
       "            (dropout3): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (1): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.3, inplace=False)\n",
       "            (dropout2): Dropout(p=0.3, inplace=False)\n",
       "            (dropout3): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (2): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.3, inplace=False)\n",
       "            (dropout2): Dropout(p=0.3, inplace=False)\n",
       "            (dropout3): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (arm): AttentionRefinementModule(\n",
       "          (conv): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          (act): ReLU(inplace=True)\n",
       "          (proj): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (post_norm): MaskBatchNorm2d(\n",
       "            (bn): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (proj): Linear(in_features=256, out_features=113, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (exprate_recorder): ExpRateRecorder()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T09:33:14.901312Z",
     "start_time": "2024-10-05T09:33:14.870061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "model.summarize()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trung\\miniconda3\\envs\\swinarm\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py:1727: LightningDeprecationWarning: Argument `mode` in `LightningModule.summarize` is deprecated in v1.4 and will be removed in v1.6. Use `max_depth=1` to replicate `mode=top` behavior.\n",
      "  f\"Argument `mode` in `LightningModule.summarize` is deprecated in v1.4\"\n",
      "\n",
      "  | Name             | Type            | Params\n",
      "-----------------------------------------------------\n",
      "0 | model            | SwinPreARM      | 30.9 M\n",
      "1 | exprate_recorder | ExpRateRecorder | 0     \n",
      "-----------------------------------------------------\n",
      "30.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "30.9 M    Total params\n",
      "123.797   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  | Name             | Type            | Params\n",
       "-----------------------------------------------------\n",
       "0 | model            | SwinPreARM      | 30.9 M\n",
       "1 | exprate_recorder | ExpRateRecorder | 0     \n",
       "-----------------------------------------------------\n",
       "30.9 M    Trainable params\n",
       "0         Non-trainable params\n",
       "30.9 M    Total params\n",
       "123.797   Total estimated model params size (MB)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T15:52:10.031869Z",
     "start_time": "2024-10-04T15:52:10.021764Z"
    }
   },
   "source": [
    "img = Image.open('original.png').convert('RGB')\n",
    "display(img)"
   ],
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (581394333.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\trung\\AppData\\Local\\Temp\\ipykernel_29400\\581394333.py\"\u001B[1;36m, line \u001B[1;32m1\u001B[0m\n\u001B[1;33m    img = Image.open('original.png').convert('RGB').\u001B[0m\n\u001B[1;37m                                                    ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "img = np.array(img)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img = comp(img)\n",
    "mask = torch.zeros_like(img[-1,:,:], dtype=torch.bool)\n",
    "hyp = model.approximate_joint_search(img.unsqueeze(0), \n",
    "                                     mask.unsqueeze(0))[0]\n",
    "pred_latex = vocab.indices2label(hyp.seq)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(pred_latex)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45d2f7d923c3019b103888df379576d885b22cd130f1a2056b8ecb4c6b8d3acc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
