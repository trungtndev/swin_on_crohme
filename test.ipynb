{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:00:09.409521Z",
     "start_time": "2024-10-07T13:00:05.917085Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from timm.models.swin_transformer import SwinTransformer"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T13:00:22.175467Z",
     "start_time": "2024-10-07T13:00:22.037977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "s = 224\n",
    "image = torch.randn(2, 3, s, s)\n",
    "custom_model = SwinTransformer(\n",
    "    img_size=s,\n",
    "    patch_size=8,\n",
    "    in_chans=3,\n",
    "    embed_dim=64,\n",
    "    depths=(2, 4, 2),\n",
    "    num_heads=(3, 8, 12),\n",
    "    window_size=(12, 6, 4),\n",
    "    mlp_ratio=4,\n",
    "\n",
    "    drop_rate=0.1,\n",
    "    proj_drop_rate=0.2,\n",
    "    attn_drop_rate=0.3,\n",
    "    drop_path_rate=0.1,\n",
    ")\n",
    "\n",
    "# custom_model.head = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(256, 512),\n",
    "#     torch.nn.LayerNorm(512),\n",
    "#     torch.nn.GELU(),\n",
    "#     torch.nn.Dropout(0.1),\n",
    "# )\n",
    "print(custom_model)\n",
    "sum(p.numel() for p in custom_model.parameters() if p.requires_grad == True)\n",
    "\n"
   ],
   "id": "b59dd276f1226e82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 64, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): SwinTransformerStage(\n",
      "      (downsample): Identity()\n",
      "      (blocks): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=64, out_features=189, bias=True)\n",
      "            (attn_drop): Dropout(p=0.3, inplace=False)\n",
      "            (proj): Linear(in_features=63, out_features=64, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.2, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (drop2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=64, out_features=189, bias=True)\n",
      "            (attn_drop): Dropout(p=0.3, inplace=False)\n",
      "            (proj): Linear(in_features=63, out_features=64, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.014)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.2, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (drop2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.014)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): SwinTransformerStage(\n",
      "      (downsample): PatchMerging(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (reduction): Linear(in_features=256, out_features=128, bias=False)\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.3, inplace=False)\n",
      "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.029)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.2, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.029)\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.3, inplace=False)\n",
      "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.043)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.2, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.043)\n",
      "        )\n",
      "        (2): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.3, inplace=False)\n",
      "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.057)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.2, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.057)\n",
      "        )\n",
      "        (3): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "            (attn_drop): Dropout(p=0.3, inplace=False)\n",
      "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.071)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.2, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.071)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): SwinTransformerStage(\n",
      "      (downsample): PatchMerging(\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=256, out_features=756, bias=True)\n",
      "            (attn_drop): Dropout(p=0.3, inplace=False)\n",
      "            (proj): Linear(in_features=252, out_features=256, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.086)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.2, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.086)\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=256, out_features=756, bias=True)\n",
      "            (attn_drop): Dropout(p=0.3, inplace=False)\n",
      "            (proj): Linear(in_features=252, out_features=256, bias=True)\n",
      "            (proj_drop): Dropout(p=0.2, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path1): DropPath(drop_prob=0.100)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.2, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (drop_path2): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): ClassifierHead(\n",
      "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      "    (flatten): Identity()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trung\\miniconda3\\envs\\swinarm\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2907432"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "nn.",
   "id": "135dc478a396387"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T13:00:26.693351Z",
     "start_time": "2024-10-07T13:00:26.547480Z"
    }
   },
   "cell_type": "code",
   "source": "custom_model(image).shape\n",
   "id": "815e17d9e94e9f54",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T09:25:11.483088Z",
     "start_time": "2024-10-06T09:25:11.436259Z"
    }
   },
   "cell_type": "code",
   "source": "custom_model.load_state_dict(model)",
   "id": "614f8abdc8d4e05d",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SwinTransformer:\n\tsize mismatch for patch_embed.proj.weight: copying a param with shape torch.Size([96, 3, 4, 4]) from checkpoint, the shape in current model is torch.Size([96, 1, 4, 4]).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_28056\\161243053.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mcustom_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_state_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\miniconda3\\envs\\swinarm\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mload_state_dict\u001B[1;34m(self, state_dict, strict)\u001B[0m\n\u001B[0;32m   1670\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merror_msgs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1671\u001B[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001B[1;32m-> 1672\u001B[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001B[0m\u001B[0;32m   1673\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0m_IncompatibleKeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmissing_keys\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0munexpected_keys\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1674\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for SwinTransformer:\n\tsize mismatch for patch_embed.proj.weight: copying a param with shape torch.Size([96, 3, 4, 4]) from checkpoint, the shape in current model is torch.Size([96, 1, 4, 4])."
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ffbd289db7af12a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
